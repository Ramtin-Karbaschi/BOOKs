# Chapter 10: Introduction to Artificial Neural Networks with Keras

## **Introduction to Neural Networks**

Artificial Neural Networks (ANNs) are computational models inspired by the human brain's biological neural networks. They consist of layers of interconnected artificial neurons that process input data to learn patterns and make predictions.

### **Key Concepts**

- **Neurons**: The basic unit of a neural network. Each neuron applies a weighted sum to its inputs and passes the result through an activation function:

$$
z=wâ‹…x+b
\newline
a=f(z)
$$

*where **$w$** is the weight vector, **$x$** is the input vector, $b$ is the bias, and $f(z)$ is the activation function.*

- **Activation Functions**: *Non-linear functions that introduce non-linearity into the model. Common activation functions include:*
    - Sigmoid:
    
    $$
    f(z)=\frac{1}{1+e^{âˆ’z}}
    $$
    
    - ReLU (*Rectified Linear Unit*):
    
    $$
    f(z)=max(0,z)
    $$
    
    - Softmax: *Used for multi-class classification, normalizes outputs into probabilities.*
- **Layers**: Neurons are organized into layers:
    - **Input Layer**: *Receives raw input data.*
    - **Hidden Layers**: *Intermediate layers that extract features.*
    - **Output Layer**: *Produces the final prediction.*

<aside>
ğŸ’¡

### **Why Neural Networks?**

Neural networks excel at modeling complex, non-linear relationships in data, making them suitable for tasks like image recognition, natural language processing, and more.

</aside>

---

## **Building Neural Networks with Keras**

Keras is a high-level API for building and training neural networks. It is integrated into TensorFlow, making it user-friendly and efficient.

### **Steps to Build a Neural Network**

1. **Define the Model** :
    
    Use **`Sequential`** for simple models or **`Functional API`** for more complex architectures.
    
    ```python
    from tensorflow import keras
    
    model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),  # Input layer
        keras.layers.Dense(300, activation="relu"),  # Hidden layer
        keras.layers.Dense(100, activation="relu"),  # Hidden layer
        keras.layers.Dense(10, activation="softmax")  # Output layer
    ])
    ```
    
2. **Compile the Model**:
    
    Specify the loss function, optimizer, and metrics.
    
    ```python
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer="sgd",  # Stochastic Gradient Descent
                  metrics=["accuracy"])
    ```
    
3. **Train the Model**:
    
    Fit the model to the training data.
    
    ```python
    history = model.fit(X_train, y_train, epochs=30, validation_split=0.1)
    ```
    
4. **Evaluate the Model**:
    
    Assess performance on test data.
    
    ```python
    test_loss, test_accuracy = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {test_accuracy}")
    ```
    

**Example:**

*MNIST Dataset*

*The chapter uses the MNIST dataset (handwritten digit images) to demonstrate how to build a neural network for classification:*

```python
from tensorflow.keras.datasets import mnist

(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]

# Normalize the data
X_train, X_valid, X_test = X_train / 255.0, X_valid / 255.0, X_test / 255.0
```

---

## **Training Deep Neural Networks**

Training deep neural networks involves addressing several challenges:

### **Challenges**

1. **Vanishing/Exploding Gradients** :
    - Gradients can become too small (vanish) or too large (explode) during backpropagation, hindering learning.
    - Solutions:
        - **Weight Initialization** : Use techniques like He initialization for ReLU activations.
        - **Normalization** : Apply batch normalization to stabilize training.
            
            ```python
            keras.layers.BatchNormalization()
            ```
            
2. **Optimization Algorithms** :
    - **SGD (Stochastic Gradient Descent)**: Basic but slow.
    - **Adam**: Combines momentum and adaptive learning rates for faster convergence.
        
        ```python
        optimizer="adam"
        ```
        
3. **Regularization** :
    - Prevent overfitting using dropout layers:
        
        ```python
        keras.layers.Dropout(rate=0.2)
        ```
        

---

## **Evaluating Model Performance**

Evaluating neural networks requires careful consideration of metrics and techniques:

### **Metrics**

- **Accuracy**: Fraction of correct predictions.
- **Confusion Matrix**: Shows true vs. predicted classifications.
- **Precision, Recall, F1-Score**: Useful for imbalanced datasets.

### **Cross-Validation**

Use cross-validation to assess model robustness:

```python
from sklearn.model_selection import cross_val_score

cross_val_score(model, X_train, y_train, cv=3, scoring="accuracy")
```

---

## **Practical Tips for Training Neural Networks**

- **Data Preprocessing** :
    - Normalize inputs to improve convergence.
    - Augment data (e.g., rotate, flip images) to increase diversity.
- **Hyperparameter Tuning** :
    - Experiment with learning rates, batch sizes, and network architectures.
- **Early Stopping** :
Stop training when validation performance plateaus to prevent overfitting:
    
    ```python
    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
    ```
    

---

## Conclusion

This chapter serves as a foundational guide to neural networks, bridging traditional machine learning techniques with deep learning. By understanding the structure of ANNs, implementing models with Keras, and addressing training challenges, readers gain the skills to tackle real-world problems.

### **Key Takeaways**

- Neural networks are powerful tools for modeling complex data.
- Keras simplifies the process of building and training models.
- Addressing challenges like vanishing gradients and overfitting is crucial for success.
- Evaluation metrics and techniques ensure reliable model performance.

# ÙØµÙ„ Û±Û°: Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Keras

## **Ù…Ù‚Ø¯Ù…Ù‡â€ŒØ§ÛŒ Ø¨Ø± Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ**

Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ (ANNs) Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø§Ø² Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù…ØºØ² Ø§Ù†Ø³Ø§Ù† Ø§Ù„Ù‡Ø§Ù… Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ØªØµÙ„ Ø¨Ù‡ Ù‡Ù… ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ùˆ Ø§Ù†Ø¬Ø§Ù… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.

### **Ù…ÙØ§Ù‡ÛŒÙ… Ú©Ù„ÛŒØ¯ÛŒ**

- **Neurons**: ÙˆØ§Ø­Ø¯ Ù¾Ø§ÛŒÙ‡ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ. Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ²Ù†â€ŒØ¯Ø§Ø± ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ ÛŒÚ© ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

$$
z=wâ‹…x+b
\newline
a=f(z)
$$

*Ú©Ù‡ Ø¯Ø± Ø¢Ù† **w** Ø¨Ø±Ø¯Ø§Ø± ÙˆØ²Ù†ØŒ **x** Ø¨Ø±Ø¯Ø§Ø± ÙˆØ±ÙˆØ¯ÛŒØŒ b Ø¨Ø§ÛŒØ§Ø³ Ùˆ f(z) ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³Øª.*

- **ØªÙˆØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Activation Functions)**: *ØªÙˆØ§Ø¨Ø¹ ØºÛŒØ±Ø®Ø·ÛŒ Ú©Ù‡ ØºÛŒØ±Ø®Ø·ÛŒ Ø¨ÙˆØ¯Ù† Ø±Ø§ Ø¨Ù‡ Ù…Ø¯Ù„ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. ØªÙˆØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ÛŒØ¬ Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²:*
    - Sigmoid:
    
    $$
    f(z)=\frac{1}{1+e^{âˆ’z}}
    $$
    
    - ReLU (*ÙˆØ§Ø­Ø¯ ÛŒÚ©Ø³ÙˆØ³Ø§Ø² Ø®Ø·ÛŒ*):
    
    $$
    f(z)=max(0,z)
    $$
    
    - Softmax: *Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ù†Ø¯ Ú©Ù„Ø§Ø³Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.*
- **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ (Layers)**: Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø¯Ø± Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:
    - **Ù„Ø§ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒ (Input Layer)**: *Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.*
    - **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† (Hidden Layers)**: *Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ÛŒØ§Ù†ÛŒ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.*
    - **Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ (Output Layer)**: *Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.*

<aside>
ğŸ’¡ **Ú†Ø±Ø§ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒØŸ**

Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¯Ø± Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ ØºÛŒØ±Ø®Ø·ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ¸Ø§ÛŒÙÛŒ Ù…Ø§Ù†Ù†Ø¯ ØªØ´Ø®ÛŒØµ ØªØµÙˆÛŒØ±ØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ù…ÙˆØ§Ø±Ø¯ Ø¯ÛŒÚ¯Ø± Ù…Ù†Ø§Ø³Ø¨ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.

</aside>

---

## **Ø³Ø§Ø®Øª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø§ Keras**

Keras ÛŒÚ© API Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± TensorFlow ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø´Ø¯Ù‡ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ù¾Ø³Ù†Ø¯ Ùˆ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø§Ø³Øª.

### **Ù…Ø±Ø§Ø­Ù„ Ø³Ø§Ø®Øª ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ**

1. **ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„**:Ø§Ø² **Sequential** Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ ÛŒØ§ **Functional API** Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.
    
    ```python
    from tensorflow import keras
    
    model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),  # Input layer
        keras.layers.Dense(300, activation="relu"),  # Hidden layer
        keras.layers.Dense(100, activation="relu"),  # Hidden layer
        keras.layers.Dense(10, activation="softmax")  # Output layer
    ])
    ```
    
2. **Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„**:ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ØŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ùˆ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯.
    
    ```python
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer="sgd",  # Stochastic Gradient Descent
                  metrics=["accuracy"])
    ```
    
3. **Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„**:Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ ØªØ·Ø¨ÛŒÙ‚ Ø¯Ù‡ÛŒØ¯.
    
    ```python
    history = model.fit(X_train, y_train, epochs=30, validation_split=0.1)
    ```
    
4. **Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„**:Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯.
    
    ```python
    test_loss, test_accuracy = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {test_accuracy}")
    ```
    

**Ù…Ø«Ø§Ù„:**

*Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ MNIST*

Ø§ÛŒÙ† ÙØµÙ„ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ MNIST (ØªØµØ§ÙˆÛŒØ± Ø§Ø±Ù‚Ø§Ù… Ø¯Ø³Øªâ€ŒÙ†ÙˆÛŒØ³) Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ø­ÙˆÙ‡ Ø³Ø§Ø®Øª ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

```python
from tensorflow.keras.datasets import mnist

(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]

# Normalize the data
X_train, X_valid, X_test = X_train / 255.0, X_valid / 255.0, X_test / 255.0
```

---

## **Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚**

Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚ Ø´Ø§Ù…Ù„ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ú†Ù†Ø¯ÛŒÙ† Ú†Ø§Ù„Ø´ Ø§Ø³Øª:

### **Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§**

1. **Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ùˆ Ø´ÙˆÙ†Ø¯Ù‡/Ù…Ù†ÙØ¬Ø± Ø´ÙˆÙ†Ø¯Ù‡**:
    - Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¯Ø± Ø·ÙˆÙ„ Ù¾Ø³â€ŒØ§Ù†ØªØ´Ø§Ø± Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú© (Ù…Ø­Ùˆ) ÛŒØ§ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ (Ù…Ù†ÙØ¬Ø±) Ø´ÙˆÙ†Ø¯ Ú©Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±Ø§ Ù…Ø®ØªÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    - Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:
        - **Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§**: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ He Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ReLU.
        - **Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ**: Ø§Ø¹Ù…Ø§Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´.
            
            ```python
            keras.layers.BatchNormalization()
            ```
            
2. **Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ**:
    - **SGD (Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù†Ø²ÙˆÙ„ÛŒ ØªØµØ§Ø¯ÙÛŒ)**: Ø³Ø§Ø¯Ù‡ Ø§Ù…Ø§ Ú©Ù†Ø¯.
    - **Adam**: ØªØ±Ú©ÛŒØ¨ Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ùˆ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±.
        
        ```python
        optimizer="adam"
        ```
        
3. **Ù…Ù†Ø¸Ù…â€ŒØ³Ø§Ø²ÛŒ**:
    - Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨ÛŒØ´â€ŒØ¨Ø±Ø§Ø²Ø´ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Dropout:
        
        ```python
        keras.layers.Dropout(rate=0.2)
        ```
        

---

## **Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„**

Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ Ø§Ø³Øª:

### **Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§**

- **Ø¯Ù‚Øª (Accuracy)**: Ù†Ø³Ø¨Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­.
- **Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix)**: Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
- **PrecisionØŒ RecallØŒ F1-Score**: Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…ØªÙˆØ§Ø²Ù† Ù…ÙÛŒØ¯ Ù‡Ø³ØªÙ†Ø¯.

### **Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªÙ‚Ø§Ø·Ø¹**

Ø§Ø² Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø³ØªØ­Ú©Ø§Ù… Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:

```python
from sklearn.model_selection import cross_val_score

cross_val_score(model, X_train, y_train, cv=3, scoring="accuracy")
```

---

## **Ù†Ú©Ø§Øª Ø¹Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ**

- **Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡**:
    - Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ.
    - Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ú†Ø±Ø®Ø´ØŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ØªØµØ§ÙˆÛŒØ±) Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ØªÙ†ÙˆØ¹.
- **ØªÙ†Ø¸ÛŒÙ… Ø§Ø¨Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§**:
    - Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¨Ø§ Ù†Ø±Ø®â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒØŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø¹Ù…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡.
- **ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…**:
ØªÙˆÙ‚Ù Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ù‡ Ø«Ø¨Ø§Øª Ù…ÛŒâ€ŒØ±Ø³Ø¯ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨ÛŒØ´â€ŒØ¨Ø±Ø§Ø²Ø´:
    
    ```python
    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
    ```
    

---

## Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

Ø§ÛŒÙ† ÙØµÙ„ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø³Ù†ØªÛŒ Ø±Ø§ Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ù¾ÛŒÙˆÙ†Ø¯ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø§ Ø¯Ø±Ú© Ø³Ø§Ø®ØªØ§Ø± ANNÙ‡Ø§ØŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Keras Ùˆ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø®ÙˆØ§Ù†Ù†Ø¯Ú¯Ø§Ù† Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø³Ø§Ø¦Ù„ Ø¯Ù†ÛŒØ§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ú©Ø³Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.

### **Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ**

- Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù‡Ø³ØªÙ†Ø¯.
- Keras ÙØ±Ø¢ÛŒÙ†Ø¯ Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø³Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ùˆ Ø´ÙˆÙ†Ø¯Ù‡ Ùˆ Ø¨ÛŒØ´â€ŒØ¨Ø±Ø§Ø²Ø´ Ø¨Ø±Ø§ÛŒ Ù…ÙˆÙÙ‚ÛŒØª Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.
- Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„ Ø±Ø§ ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.

Ø¨Ø§ ØªØ³Ù„Ø· Ø¨Ø± Ø§ÛŒÙ† Ù…ÙØ§Ù‡ÛŒÙ…ØŒ Ø´Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ø¢Ù…Ø§Ø¯Ù‡ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø¨ÙˆØ¯.